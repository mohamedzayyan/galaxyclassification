{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./pytorch_sol2/')\n",
    "#sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os, time, copy, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from skimage import io, transform\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import pickle\n",
    "from PIL import Image\n",
    "\n",
    "from config import *\n",
    "from GalaxiesDataset import *\n",
    "from rsa_loader import *\n",
    "from efigi_loader import *\n",
    "from pytorchtools import EarlyStopping\n",
    "\n",
    "import pickle\n",
    "from DatasetFromSubset import *\n",
    "from samplers import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pandas_ml import ConfusionMatrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from mySiamese import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transf = transforms.Compose([transforms.Resize((105, 105)),\n",
    "                             transforms.RandomHorizontalFlip(p=0.5),\n",
    "                             transforms.RandomRotation(degrees=(0,360)),\n",
    "                             transforms.RandomVerticalFlip(p=0.5),\n",
    "                             transforms.ToTensor(),\n",
    "                             transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                  std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "transformations = transforms.Compose([transforms.Resize((105, 105)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = GalaxiesDataset_efigi(EFIGI_DIR, EFIGI_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = []\n",
    "for row in range(train_ds.classes_frame.shape[0]):\n",
    "    cl.append(np.argmax(train_ds.classes_frame.loc[:,train_ds.classes_frame.columns[3:]].loc[row]))\n",
    "\n",
    "temp = pd.DataFrame(cl,columns=['cat'])\n",
    "temp['cat'] = temp['cat'].astype('category')\n",
    "temp['cat'] = temp['cat'].cat.codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.from_numpy(temp.values)\n",
    "train_idx, valid_idx= train_test_split(\n",
    "np.arange(len(y)),\n",
    "test_size=0.4,\n",
    "shuffle=True,\n",
    "    random_state=42,\n",
    "stratify=y)\n",
    "\n",
    "valid_idx, test_idx= train_test_split(\n",
    "np.arange(len(y[valid_idx])),\n",
    "test_size=0.5,\n",
    "shuffle=True,\n",
    "    random_state=42,\n",
    "stratify=y[valid_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "new_train = []\n",
    "testers = []\n",
    "y_train = y[train_idx].reshape((-1, ))\n",
    "for i in range(0,12):\n",
    "    l = i\n",
    "    inds = np.argwhere(y_train == l)[:,0]\n",
    "    testers.append(train_idx[inds[0]])\n",
    "    for j in range(1, inds.shape[0]):\n",
    "        new_train.append(train_idx[inds[j]])\n",
    "new_train = np.array(new_train)\n",
    "np.random.shuffle(new_train)\n",
    "testers = np.array(testers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 3122 Train_dl: 1 Validation_dl: 1 Test_dl: 1\n"
     ]
    }
   ],
   "source": [
    "size = len(train_ds)\n",
    "indices = list(range(size))\n",
    "split = int(np.floor(VALIDATION_SPLIT * size))\n",
    "if SHUFFLE_DS:\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "test_split = int(np.floor(VALIDATION_SPLIT * size))\n",
    "if SHUFFLE_DS:\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, test_indices = train_indices[test_split:], train_indices[:test_split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "val_sampler   = SubsetRandomSampler(valid_idx)\n",
    "test_sampler = SubsetRandomSampler(test_idx)\n",
    "\n",
    "tng = Subset(train_ds, train_idx)\n",
    "val = Subset(train_ds, valid_idx)\n",
    "test = Subset(train_ds, test_idx) \n",
    "#base = Subset(train_ds, testers)\n",
    "\n",
    "tng_dataset = DatasetFromSubset(tng, transf)\n",
    "val_dataset = DatasetFromSubset(val, transformations)\n",
    "test_dataset = DatasetFromSubset(test, transformations)\n",
    "#base_ds = DatasetFromSubset(base, transformations)\n",
    "\n",
    "\n",
    "train_dl = DataLoader(tng_dataset, batch_size=len(train_idx), num_workers=0)\n",
    "val_dl = DataLoader(val_dataset, batch_size=len(valid_idx), num_workers=0)\n",
    "unseen_dl = DataLoader(test_dataset, batch_size=len(test_idx), num_workers=0)\n",
    "#base_dl = DataLoader(base_ds, batch_size=len(testers), num_workers=0)\n",
    "\n",
    "#train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, num_workers=0,\n",
    "#                                                 sampler=train_sampler)\n",
    "#val_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, num_workers=0,\n",
    "#                                                 sampler=val_sampler)\n",
    "#unseen_dl = DataLoader(train_ds,batch_size=BATCH_SIZE, num_workers=0,\n",
    "#                                                 sampler=test_sampler)\n",
    "print(\"Total: {} Train_dl: {} Validation_dl: {} Test_dl: {}\".format(size, len(train_dl),\n",
    "                                                                    len(val_dl),\n",
    "                                                                   len(unseen_dl)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "624"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dl.sampler.data_source.subset.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(base_dl.sampler.data_source.subset.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "625"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unseen_dl.sampler.data_source.subset.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1873"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dl.sampler.data_source.subset.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To just evaluate, use tr='val' \n",
    "tr = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tr == 'train':\n",
    "    model = torch.load('./models/gz2_resnet50')\n",
    "if tr == 'val':\n",
    "    model = torch.load('./models/efigiSiamesePair')\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, weight_decay=0.00005)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min',patience=5,threshold=0.0001,factor=0.1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class parameters ###\n",
    "### For random sampling: mining='random' ### \n",
    "### For triplet loss: modelType='triplet' ###\n",
    "### For contrastive loss: modelType='pair' ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training starting\n",
      "\n",
      "epoch: 0\n",
      "Train loss: 0.9956789036591848, Val loss: 0.8349508941173553\n",
      "Validation loss decreased (inf --> 0.834951).  Saving model ...\n",
      "\n",
      "epoch: 1\n",
      "Train loss: 0.9414457003275554, Val loss: 0.782412999868393\n",
      "Validation loss decreased (0.834951 --> 0.782413).  Saving model ...\n",
      "\n",
      "epoch: 2\n",
      "Train loss: 0.9061650534470876, Val loss: 0.7401562511920929\n",
      "Validation loss decreased (0.782413 --> 0.740156).  Saving model ...\n",
      "\n",
      "epoch: 3\n",
      "Train loss: 0.8612610419591268, Val loss: 0.7066856563091278\n",
      "Validation loss decreased (0.740156 --> 0.706686).  Saving model ...\n",
      "\n",
      "epoch: 4\n",
      "Train loss: 0.8255460957686106, Val loss: 0.6765064179897309\n",
      "Validation loss decreased (0.706686 --> 0.676506).  Saving model ...\n",
      "\n",
      "epoch: 5\n",
      "Train loss: 0.7950588782628377, Val loss: 0.6501428008079528\n",
      "Validation loss decreased (0.676506 --> 0.650143).  Saving model ...\n",
      "\n",
      "epoch: 6\n",
      "Train loss: 0.763510282834371, Val loss: 0.6271258115768432\n",
      "Validation loss decreased (0.650143 --> 0.627126).  Saving model ...\n",
      "\n",
      "epoch: 7\n",
      "Train loss: 0.7396992941697439, Val loss: 0.6045004665851593\n",
      "Validation loss decreased (0.627126 --> 0.604500).  Saving model ...\n",
      "\n",
      "epoch: 8\n",
      "Train loss: 0.7108162879943848, Val loss: 0.5837593913078308\n",
      "Validation loss decreased (0.604500 --> 0.583759).  Saving model ...\n",
      "\n",
      "epoch: 9\n",
      "Train loss: 0.6904475053151449, Val loss: 0.564980936050415\n",
      "Validation loss decreased (0.583759 --> 0.564981).  Saving model ...\n",
      "\n",
      "epoch: 10\n",
      "Train loss: 0.6745897769927979, Val loss: 0.5478390038013459\n",
      "Validation loss decreased (0.564981 --> 0.547839).  Saving model ...\n",
      "\n",
      "epoch: 11\n",
      "Train loss: 0.6553097347418467, Val loss: 0.5340848654508591\n",
      "Validation loss decreased (0.547839 --> 0.534085).  Saving model ...\n",
      "\n",
      "epoch: 12\n",
      "Train loss: 0.6375706613063812, Val loss: 0.5225542992353439\n",
      "Validation loss decreased (0.534085 --> 0.522554).  Saving model ...\n",
      "\n",
      "epoch: 13\n",
      "Train loss: 0.6369884729385376, Val loss: 0.5124975621700287\n",
      "Validation loss decreased (0.522554 --> 0.512498).  Saving model ...\n",
      "\n",
      "epoch: 14\n",
      "Train loss: 0.6177664856115977, Val loss: 0.5032452583312989\n",
      "Validation loss decreased (0.512498 --> 0.503245).  Saving model ...\n",
      "\n",
      "epoch: 15\n",
      "Train loss: 0.6149141212304433, Val loss: 0.49450730681419375\n",
      "Validation loss decreased (0.503245 --> 0.494507).  Saving model ...\n",
      "\n",
      "epoch: 16\n",
      "Train loss: 0.6174068421125412, Val loss: 0.48670979142189025\n",
      "Validation loss decreased (0.494507 --> 0.486710).  Saving model ...\n",
      "\n",
      "epoch: 17\n",
      "Train loss: 0.6025273005167643, Val loss: 0.4799144953489304\n",
      "Validation loss decreased (0.486710 --> 0.479914).  Saving model ...\n",
      "\n",
      "epoch: 18\n",
      "Train loss: 0.5936626434326172, Val loss: 0.4735150098800659\n",
      "Validation loss decreased (0.479914 --> 0.473515).  Saving model ...\n",
      "\n",
      "epoch: 19\n",
      "Train loss: 0.5773764878511429, Val loss: 0.46743335723876955\n",
      "Validation loss decreased (0.473515 --> 0.467433).  Saving model ...\n",
      "\n",
      "epoch: 20\n",
      "Train loss: 0.5773919562498728, Val loss: 0.4618451505899429\n",
      "Validation loss decreased (0.467433 --> 0.461845).  Saving model ...\n",
      "\n",
      "epoch: 21\n",
      "Train loss: 0.5784179439147313, Val loss: 0.4563678175210953\n",
      "Validation loss decreased (0.461845 --> 0.456368).  Saving model ...\n",
      "\n",
      "epoch: 22\n",
      "Train loss: 0.5695277531941731, Val loss: 0.45086815059185026\n",
      "Validation loss decreased (0.456368 --> 0.450868).  Saving model ...\n",
      "\n",
      "epoch: 23\n",
      "Train loss: 0.5640388896067937, Val loss: 0.4449124425649643\n",
      "Validation loss decreased (0.450868 --> 0.444912).  Saving model ...\n",
      "\n",
      "epoch: 24\n",
      "Train loss: 0.5519254247347514, Val loss: 0.43975571990013124\n",
      "Validation loss decreased (0.444912 --> 0.439756).  Saving model ...\n",
      "\n",
      "epoch: 25\n",
      "Train loss: 0.5513137658437093, Val loss: 0.4344232171773911\n",
      "Validation loss decreased (0.439756 --> 0.434423).  Saving model ...\n",
      "\n",
      "epoch: 26\n",
      "Train loss: 0.5450456728537877, Val loss: 0.4291154831647873\n",
      "Validation loss decreased (0.434423 --> 0.429115).  Saving model ...\n",
      "\n",
      "epoch: 27\n",
      "Train loss: 0.545406065384547, Val loss: 0.42373456060886383\n",
      "Validation loss decreased (0.429115 --> 0.423735).  Saving model ...\n",
      "\n",
      "epoch: 28\n",
      "Train loss: 0.5407236675421397, Val loss: 0.4190272778272629\n",
      "Validation loss decreased (0.423735 --> 0.419027).  Saving model ...\n",
      "\n",
      "epoch: 29\n",
      "Train loss: 0.5343818763891856, Val loss: 0.4141279637813568\n",
      "Validation loss decreased (0.419027 --> 0.414128).  Saving model ...\n",
      "\n",
      "epoch: 30\n",
      "Train loss: 0.5254607766866684, Val loss: 0.4090982794761658\n",
      "Validation loss decreased (0.414128 --> 0.409098).  Saving model ...\n",
      "\n",
      "epoch: 31\n",
      "Train loss: 0.5203356524308522, Val loss: 0.4043027698993683\n",
      "Validation loss decreased (0.409098 --> 0.404303).  Saving model ...\n",
      "\n",
      "epoch: 32\n",
      "Train loss: 0.5162724673748016, Val loss: 0.3998608261346817\n",
      "Validation loss decreased (0.404303 --> 0.399861).  Saving model ...\n",
      "\n",
      "epoch: 33\n",
      "Train loss: 0.5194884876410166, Val loss: 0.39469075202941895\n",
      "Validation loss decreased (0.399861 --> 0.394691).  Saving model ...\n",
      "\n",
      "epoch: 34\n",
      "Train loss: 0.5150620271762212, Val loss: 0.3898922234773636\n",
      "Validation loss decreased (0.394691 --> 0.389892).  Saving model ...\n",
      "\n",
      "epoch: 35\n",
      "Train loss: 0.5143781473239263, Val loss: 0.38503901064395907\n",
      "Validation loss decreased (0.389892 --> 0.385039).  Saving model ...\n",
      "\n",
      "epoch: 36\n",
      "Train loss: 0.5033475195368131, Val loss: 0.37962594628334045\n",
      "Validation loss decreased (0.385039 --> 0.379626).  Saving model ...\n",
      "\n",
      "epoch: 37\n",
      "Train loss: 0.5035726646582286, Val loss: 0.37510600686073303\n",
      "Validation loss decreased (0.379626 --> 0.375106).  Saving model ...\n",
      "\n",
      "epoch: 38\n",
      "Train loss: 0.49218769520521166, Val loss: 0.3706814289093018\n",
      "Validation loss decreased (0.375106 --> 0.370681).  Saving model ...\n",
      "\n",
      "epoch: 39\n",
      "Train loss: 0.49173111816247306, Val loss: 0.36591909229755404\n",
      "Validation loss decreased (0.370681 --> 0.365919).  Saving model ...\n",
      "\n",
      "epoch: 40\n",
      "Train loss: 0.4904549439748128, Val loss: 0.36119588315486906\n",
      "Validation loss decreased (0.365919 --> 0.361196).  Saving model ...\n",
      "\n",
      "epoch: 41\n",
      "Train loss: 0.48601703296105064, Val loss: 0.3565611749887466\n",
      "Validation loss decreased (0.361196 --> 0.356561).  Saving model ...\n",
      "\n",
      "epoch: 42\n",
      "Train loss: 0.4808870702981949, Val loss: 0.35189844369888307\n",
      "Validation loss decreased (0.356561 --> 0.351898).  Saving model ...\n",
      "\n",
      "epoch: 43\n",
      "Train loss: 0.48246873219807945, Val loss: 0.3470395505428314\n",
      "Validation loss decreased (0.351898 --> 0.347040).  Saving model ...\n",
      "\n",
      "epoch: 44\n",
      "Train loss: 0.47665386895338696, Val loss: 0.34165545403957365\n",
      "Validation loss decreased (0.347040 --> 0.341655).  Saving model ...\n",
      "\n",
      "epoch: 45\n",
      "Train loss: 0.46418828268845874, Val loss: 0.3371159493923187\n",
      "Validation loss decreased (0.341655 --> 0.337116).  Saving model ...\n",
      "\n",
      "epoch: 46\n",
      "Train loss: 0.4754099210103353, Val loss: 0.332549712061882\n",
      "Validation loss decreased (0.337116 --> 0.332550).  Saving model ...\n",
      "\n",
      "epoch: 47\n",
      "Train loss: 0.46553921699523926, Val loss: 0.3280546098947525\n",
      "Validation loss decreased (0.332550 --> 0.328055).  Saving model ...\n",
      "\n",
      "epoch: 48\n",
      "Train loss: 0.4618564397096634, Val loss: 0.3233442008495331\n",
      "Validation loss decreased (0.328055 --> 0.323344).  Saving model ...\n",
      "\n",
      "epoch: 49\n",
      "Train loss: 0.4585680196682612, Val loss: 0.3192082330584526\n",
      "Validation loss decreased (0.323344 --> 0.319208).  Saving model ...\n",
      "\n",
      "epoch: 50\n",
      "Train loss: 0.45568308035532634, Val loss: 0.31463264375925065\n",
      "Validation loss decreased (0.319208 --> 0.314633).  Saving model ...\n",
      "\n",
      "epoch: 51\n",
      "Train loss: 0.45109410136938094, Val loss: 0.3104633554816246\n",
      "Validation loss decreased (0.314633 --> 0.310463).  Saving model ...\n",
      "\n",
      "epoch: 52\n",
      "Train loss: 0.45018952041864396, Val loss: 0.3058310315012932\n",
      "Validation loss decreased (0.310463 --> 0.305831).  Saving model ...\n",
      "\n",
      "epoch: 53\n",
      "Train loss: 0.4426624685525894, Val loss: 0.30230678617954254\n",
      "Validation loss decreased (0.305831 --> 0.302307).  Saving model ...\n",
      "\n",
      "epoch: 54\n",
      "Train loss: 0.4443803275624911, Val loss: 0.2979216888546944\n",
      "Validation loss decreased (0.302307 --> 0.297922).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "mySiam = mySiamese(train_dl=train_dl, val_dl=val_dl, unseen_dl=unseen_dl, mining='random', model=model,\n",
    "                   margin=1.2, outputSize=128,optimizer=optimizer,scheduler=scheduler,\n",
    "                  modelType='triplet', device=DEVICE)\n",
    "if tr == 'train':\n",
    "    mySiam.train(epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parameters ###\n",
    "### For weighted F1: f1_ = 'weighted' ###\n",
    "### For unweighted F1: f1_ = 'notWeighted' ###\n",
    "### For numbers cf: cf_type ='numbers' ###\n",
    "### For percentage cf: cf_type='percentage' ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mySiam.evaluate2()\n",
    "#model, testPreds, trainPreds, distMatrix, train_label_anchor, predLabels, actLabels, avg_fts = mySiam.evaluate2(epochs=5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
